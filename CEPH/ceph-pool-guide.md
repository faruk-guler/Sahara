# ğŸ± Ceph Pool YÃ¶netimi Rehberi

Bu dokÃ¼man, Ceph havuzlarÄ±nÄ±n (pool) oluÅŸturulmasÄ±, yapÄ±landÄ±rÄ±lmasÄ± ve yÃ¶netimini kapsar. Pool'lar, Ceph'te verilerin organize edildiÄŸi mantÄ±ksal birimlerdir.

---

## 1. Pool Temelleri

### Pool Nedir?

Pool, Ceph'te nesnelerin (objects) gruplandÄ±ÄŸÄ± mantÄ±ksal bir kapsayÄ±cÄ±dÄ±r. Her pool'un kendine Ã¶zgÃ¼:

- Replikasyon veya Erasure Coding ayarÄ±
- Placement Group (PG) sayÄ±sÄ±
- CRUSH kuralÄ± (hangi OSD'lere yazÄ±lacaÄŸÄ±)
- Uygulama etiketi (rbd, cephfs, rgw)

### Pool Listesini GÃ¶rme

```bash
# Basit liste
ceph osd lspools

# DetaylÄ± bilgi
ceph osd pool ls detail

# Ä°statistiklerle birlikte
ceph df
```

---

## 2. Replicated Pool OluÅŸturma

### Temel OluÅŸturma

```bash
# SÃ¶zdizimi: ceph osd pool create <pool-adÄ±> <pg-sayÄ±sÄ±>
ceph osd pool create mypool 128

# Uygulama etiketi ekle (zorunlu)
ceph osd pool application enable mypool rbd
```

### Parametreli OluÅŸturma

```bash
# Replikasyon sayÄ±sÄ±nÄ± belirterek
ceph osd pool create mypool 128 128 replicated
ceph osd pool set mypool size 3        # 3 kopya
ceph osd pool set mypool min_size 2    # Minimum 2 kopya yazÄ±lmalÄ±
```

### PG SayÄ±sÄ± Hesaplama

```text
PG SayÄ±sÄ± = (OSD SayÄ±sÄ± Ã— 100) Ã· Replikasyon SayÄ±sÄ±

Ã–rnek: 9 OSD, size=3 â†’ (9 Ã— 100) Ã· 3 = 300 â†’ En yakÄ±n 2'nin kuvveti = 256
```

> **Pro Tip:** Ceph Reef'te `pg_autoscaler` varsayÄ±lan olarak aÃ§Ä±ktÄ±r. Manuel hesaplamaya gerek kalmaz.

```bash
# Autoscaler durumunu kontrol et
ceph osd pool autoscale-status
```

---

## 3. Erasure Coded (EC) Pool OluÅŸturma

EC, replikasyondan daha verimli disk kullanÄ±mÄ± saÄŸlar ancak yazma performansÄ± dÃ¼ÅŸer.

### EC Profili OluÅŸturma

```bash
# k=4, m=2 profili (4 veri + 2 parite = 6 parÃ§a)
# 2 OSD bozulsa bile veri kurtarÄ±labilir
ceph osd erasure-code-profile set my-ec-profile k=4 m=2 crush-failure-domain=host

# Profili gÃ¶rÃ¼ntÃ¼le
ceph osd erasure-code-profile get my-ec-profile
```

### EC Pool OluÅŸturma

```bash
ceph osd pool create ec-pool 128 128 erasure my-ec-profile
ceph osd pool application enable ec-pool rgw
```

### EC vs Replicated KarÅŸÄ±laÅŸtÄ±rma

| Ã–zellik | Replicated (size=3) | EC (4+2) |
| :--- | :--- | :--- |
| Disk VerimliliÄŸi | %33 | %66 |
| Yazma HÄ±zÄ± | HÄ±zlÄ± | YavaÅŸ (CPU yoÄŸun) |
| Okuma HÄ±zÄ± | HÄ±zlÄ± | Orta |
| Recovery SÃ¼resi | KÄ±sa | Uzun |
| KullanÄ±m AlanÄ± | RBD, CephFS | ArÅŸiv, Yedekleme, RGW |

---

## 4. Pool YapÄ±landÄ±rmasÄ±

### Replikasyon AyarlarÄ±

```bash
# Kopya sayÄ±sÄ±nÄ± deÄŸiÅŸtir
ceph osd pool set mypool size 3

# Minimum yazÄ±labilir kopya (size'dan kÃ¼Ã§Ã¼k olmalÄ±)
ceph osd pool set mypool min_size 2
```

### PG SayÄ±sÄ±nÄ± DeÄŸiÅŸtirme

```bash
# PG artÄ±rma (azaltma Reef'ten itibaren desteklenir)
ceph osd pool set mypool pg_num 256

# Autoscaler modunu ayarla
ceph osd pool set mypool pg_autoscale_mode on    # Otomatik
ceph osd pool set mypool pg_autoscale_mode warn  # Sadece uyarÄ±
ceph osd pool set mypool pg_autoscale_mode off   # KapalÄ±
```

### Pool QuotasÄ±

```bash
# Maksimum boyut limiti (100 GB)
ceph osd pool set-quota mypool max_bytes 107374182400

# Maksimum nesne sayÄ±sÄ± limiti
ceph osd pool set-quota mypool max_objects 1000000

# Quota'yÄ± kaldÄ±r
ceph osd pool set-quota mypool max_bytes 0
```

---

## 5. Pool Silme

> âš ï¸ **DÄ°KKAT:** Pool silme geri alÄ±namaz ve tÃ¼m verileri kalÄ±cÄ± olarak siler!

### GÃ¼venlik Kilidi AÃ§ma

```bash
# Ã–nce config'de pool silmeye izin ver
ceph config set mon mon_allow_pool_delete true
```

### Pool Silme

```bash
# Ä°ki kez pool adÄ±nÄ± yazarak sil
ceph osd pool delete mypool mypool --yes-i-really-really-mean-it
```

### GÃ¼venlik Kilidini Tekrar Kapat

```bash
ceph config set mon mon_allow_pool_delete false
```

---

## 6. Pool Yeniden AdlandÄ±rma

```bash
ceph osd pool rename old-pool-name new-pool-name
```

---

## 7. Pool Snapshot

### Snapshot OluÅŸturma

```bash
ceph osd pool mksnap mypool snap1
```

### Snapshot Listeleme

```bash
rados -p mypool lssnap
```

### Snapshot Silme

```bash
ceph osd pool rmsnap mypool snap1
```

---

## 8. Pool Compression

BlueStore ile pool seviyesinde sÄ±kÄ±ÅŸtÄ±rma aktif edilebilir.

```bash
# SÄ±kÄ±ÅŸtÄ±rma algoritmasÄ± ayarla
ceph osd pool set mypool compression_algorithm snappy  # veya lz4, zstd, zlib

# SÄ±kÄ±ÅŸtÄ±rma modunu ayarla
ceph osd pool set mypool compression_mode aggressive  # none, passive, aggressive, force

# Minimum sÄ±kÄ±ÅŸtÄ±rma boyutu (varsayÄ±lan 0)
ceph osd pool set mypool compression_min_blob_size 4096
```

### SÄ±kÄ±ÅŸtÄ±rma Ä°statistikleri

```bash
ceph osd pool stats mypool
```

---

## 9. Cache Tiering (Ã–nbellek KatmanÄ±)

> **Not:** Cache tiering artÄ±k Ã¶nerilmiyor. Yerine SSD-based pool + CRUSH rules kullanÄ±n.

Yine de legacy bilgi olarak:

```bash
# Cache pool oluÅŸtur (SSD'lerde)
ceph osd pool create cache-pool 128

# Cache tier olarak baÄŸla
ceph osd tier add data-pool cache-pool
ceph osd tier cache-mode cache-pool writeback
ceph osd tier set-overlay data-pool cache-pool
```

---

## 10. Pool Ä°statistikleri ve Ä°zleme

```bash
# Pool kullanÄ±m istatistikleri
ceph df detail

# Pool baÅŸÄ±na IOPS ve throughput
ceph osd pool stats

# Belirli pool iÃ§in detay
ceph osd pool get mypool all
```

---

## 11. CRUSH KuralÄ±nÄ± DeÄŸiÅŸtirme

```bash
# Pool'un kullandÄ±ÄŸÄ± CRUSH kuralÄ±nÄ± deÄŸiÅŸtir
ceph osd pool set mypool crush_rule hdd-rule
```

---

## 12. Pool Best Practices

### âœ… YapÄ±lmasÄ± Gerekenler

- Her pool'a uygulama etiketi ekleyin (`rbd`, `cephfs`, `rgw`)
- Production'da `pg_autoscaler` kullanÄ±n
- Pool silmeden Ã¶nce snapshot alÄ±n
- FarklÄ± iÅŸ yÃ¼kleri iÃ§in ayrÄ± pool'lar oluÅŸturun

### âŒ YapÄ±lmamasÄ± Gerekenler

- Tek bir pool'da tÃ¼m verileri depolamayÄ±n
- PG sayÄ±sÄ±nÄ± Ã§ok dÃ¼ÅŸÃ¼k tutmayÄ±n (performans dÃ¼ÅŸer)
- EC pool'u yoÄŸun yazma gerektiren iÅŸ yÃ¼kleri iÃ§in kullanmayÄ±n
- `min_size=1` ayarlamayÄ±n (veri kaybÄ± riski)

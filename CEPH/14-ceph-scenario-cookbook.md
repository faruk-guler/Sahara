# ğŸ³ Ceph Senaryo KitabÄ± (Scenario Cookbook)

Bu dokÃ¼man, teorik bilgilerden arÄ±ndÄ±rÄ±lmÄ±ÅŸ, doÄŸrudan **"NasÄ±l YapÄ±lÄ±r?"** sorusunu cevaplayan, kopyala-yapÄ±ÅŸtÄ±r yapmaya hazÄ±r komut reÃ§etelerini iÃ§erir.

> **Ã–n KoÅŸul:** TÃ¼m komutlar `cephadm shell` iÃ§erisinde veya `client.admin` yetkisine sahip bir terminalde Ã§alÄ±ÅŸtÄ±rÄ±lmalÄ±dÄ±r.

---

## ğŸ—ï¸ Senaryo 1: SanallaÅŸtÄ±rma iÃ§in Disk AlanÄ± (Proxmox / VMware)

**Hedef:** Sanal makineler (VM) iÃ§in yÃ¼ksek performanslÄ±, replikasyonlu (3 kopya) bir depolama alanÄ± oluÅŸturmak ve sunucuya tanÄ±tmak.

### ReÃ§ete

```bash
# 1. 'vms' adÄ±nda bir havuz oluÅŸtur (128 PG, SSD odaklÄ± olmasÄ± iÃ§in Ã¶nerilir)
ceph osd pool create vms 128 128

# 2. Havuzu RBD (Blok CihazÄ±) olarak etiketle (ZORUNLU)
ceph osd pool application enable vms rbd

# 3. 100 GB'lÄ±k bir sanal disk (Image) oluÅŸtur
rbd create vm-100-disk-1 --size 100G --pool vms

# 4. Diski listele
rbd ls -p vms -l

# 5. (Opsiyonel) Diski bu sunucuya baÄŸla (Map)
rbd map vms/vm-100-disk-1
# Ã‡Ä±ktÄ±: /dev/rbd0
```

---

## ğŸš€ Senaryo 2: VeritabanÄ± iÃ§in YÃ¼ksek Performans (SSD/NVMe Only)

**Hedef:** PostgreSQL/MySQL gibi veritabanlarÄ± iÃ§in sadece SSD veya NVMe diskleri kullanan, gecikmesi dÃ¼ÅŸÃ¼k Ã¶zel bir alan.

### ReÃ§ete

```bash
# 1. Ã–zel bir CRUSH kuralÄ± oluÅŸtur (Sadece SSD'leri kullansÄ±n)
ceph osd crush rule create-replicated rule-ssd default host ssd

# 2. Bu kuralÄ± kullanan 'db_wal' havuzunu oluÅŸtur
ceph osd pool create db_wal 32 32
ceph osd pool set db_wal crush_rule rule-ssd

# 3. Havuzu RBD olarak iÅŸaretle
ceph osd pool application enable db_wal rbd

# 4. PerformanslÄ± disk imajÄ± oluÅŸtur
rbd create postgres-wal --size 50G --pool db_wal

# 5. Linux'a baÄŸla ve performans ayarlarÄ±yla mount et
rbd map db_wal/postgres-wal
mkfs.xfs /dev/rbd1
mount -o noatime,nodiratime,discard /dev/rbd1 /var/lib/postgresql/wal
```

---

## ğŸ“‚ Senaryo 3: Ortak Dosya PaylaÅŸÄ±mÄ± (Team Share / NAS)

**Hedef:** 50 kiÅŸilik bir yazÄ±lÄ±m ekibi iÃ§in 10 TB kotalÄ±, herkesin aynÄ± anda yazabildiÄŸi ortak klasÃ¶r.

### ReÃ§ete

```bash
# 1. 'team_fs' adÄ±nda bir dosya sistemi oluÅŸtur
ceph fs volume create team_fs

# 2. (Opsiyonel) KotayÄ± ayarla (KÃ¶k dizine 10 TB limit)
# Ã–nce mount edilmeli veya MDS Ã¼zerinden ayarlanmalÄ±. Burada mount ediyoruz:
mkdir -p /mnt/team_share
mount -t ceph 192.168.1.10:6789:/ /mnt/team_share -o name=admin,secret=$(ceph auth get-key client.admin),fs=team_fs

# 3. KotayÄ± koy (Byte cinsinden 10 TB)
setfattr -n ceph.quota.max_bytes -v 10995116277760 /mnt/team_share

# 4. KullanÄ±cÄ±lar iÃ§in eriÅŸim anahtarÄ± oluÅŸtur (Sadece /mnt/team_share dizinine yetkili)
ceph fs authorize team_fs client.developers / rw
```

---

## ğŸ’¾ Senaryo 4: Ucuz Yedekleme AlanÄ± (Erasure Coding)

**Hedef:** Veeam veya arÅŸiv verileri iÃ§in diskten tasarruf eden (RAID 6 benzeri) ama biraz daha yavaÅŸ bir alan oluÅŸturmak. Veri 3 kopya yerine %66 (4+2) yer kaplayacak.

### ReÃ§ete

```bash
# 1. Erasure Code profili oluÅŸtur (4 veri + 2 parite = 1 sunucu Ã¶lse de Ã§alÄ±ÅŸÄ±r)
ceph osd erasure-code-profile set ec-backup-profile k=4 m=2 crush-failure-domain=host

# 2. Bu profili kullanan 'backups' havuzunu oluÅŸtur
ceph osd pool create backups 128 128 erasure ec-backup-profile

# 3. Havuzu RGW (Object Storage) iÃ§in etkinleÅŸtir (EC genelde RGW ile kullanÄ±lÄ±r)
ceph osd pool application enable backups rgw

# 4. RGW'ye bu havuzu kullanmasÄ±nÄ± sÃ¶yle (Zone config gerektirir, basitÃ§e manuel bucket aÃ§arken):
# Not: Modern Ceph'te bu iÅŸlem Zone Placement ayarlarÄ±yla yapÄ±lÄ±r.
# Basit kullanÄ±m iÃ§in bu havuzu bir RBD pool gibi de kullanabilirsiniz:
ceph osd pool application enable backups rbd
```

---

## ğŸ”’ Senaryo 5: Ransomware KorumasÄ± (WORM / Immutable)

**Hedef:** Yedeklenen verilerin 30 gÃ¼n boyunca "root" dahil kimse tarafÄ±ndan silinememesi (Fidye yazÄ±lÄ±mÄ± korumasÄ±).

### ReÃ§ete

> **Dikkat:** RGW (Object Gateway) kurulu olmalÄ±dÄ±r.

```bash
# 1. Object Lock Ã¶zellikli bir S3 Bucket oluÅŸtur (AWS CLI ile)
aws --endpoint-url http://ceph-ip:8000 s3api create-bucket \
    --bucket guvenli-yedek \
    --object-lock-enabled-for-bucket

# 2. Koruma kuralÄ±nÄ± koy (Compliance Modu = Kimse silemez)
aws --endpoint-url http://ceph-ip:8000 s3api put-object-lock-configuration \
    --bucket guvenli-yedek \
    --object-lock-configuration '{"ObjectLockEnabled": "Enabled", "Rule": {"DefaultRetention": {"Mode": "COMPLIANCE", "Days": 30}}}'

# 3. Test et (DosyayÄ± silmeyi dene - Hata almalÄ±sÄ±n)
aws --endpoint-url http://ceph-ip:8000 s3 rm s3://guvenli-yedek/test-dosyasi.txt
# Hata: AccessDenied
```

---

## ğŸ“¹ Senaryo 6: GÃ¼venlik KamerasÄ± KayÄ±tlarÄ± (CCTV)

**Hedef:** SÃ¼rekli yazma (Sequential Write) yapÄ±lan ve eski kayÄ±tlarÄ±n otomatik silindiÄŸi bir alan.

### ReÃ§ete

```bash
# 1. Kamera kayÄ±tlarÄ± iÃ§in bucket oluÅŸtur
aws --endpoint-url http://ceph-ip:8000 s3 mb s3://cctv-kayitlari

# 2. Lifecycle Policy (YaÅŸam DÃ¶ngÃ¼sÃ¼) oluÅŸtur - 90 gÃ¼nden eskileri sil
# policy.json dosyasÄ±:
# {
#     "Rules": [{
#         "ID": "EskiKayitlariSil",
#         "Status": "Enabled",
#         "Filter": { "Prefix": "" },
#         "Expiration": { "Days": 90 }
#     }]
# }

# 3. PolitikayÄ± uygula
aws --endpoint-url http://ceph-ip:8000 s3api put-bucket-lifecycle-configuration \
    --bucket cctv-kayitlari \
    --lifecycle-configuration file://policy.json
```

---

## ğŸ’ Senaryo 7: Enterprise "All-Flash" Deneyimi (High-End Ã–zellikler)

Pure Storage veya NetApp All-Flash dizilerinde alÄ±ÅŸÄ±k olduÄŸunuz o "Premium" Ã¶zellikleri Ceph'te nasÄ±l yaparsÄ±nÄ±z?

### A. "Always-On" SÄ±kÄ±ÅŸtÄ±rma (Compression)

Veriyi diske yazmadan Ã¶nce havada sÄ±kÄ±ÅŸtÄ±rarak yer tasarrufu saÄŸlar. CPU kullanÄ±mÄ± artar ama disk alanÄ± kazanÄ±lÄ±r (All-Flash iÃ§in ideal).

```bash
# 1. Havuzda sÄ±kÄ±ÅŸtÄ±rmayÄ± "aggressive" modda aÃ§ (Her ÅŸeyi sÄ±kÄ±ÅŸtÄ±r)
ceph osd pool set vms compression_mode aggressive

# 2. AlgoritmayÄ± seÃ§ (lz4 = HÄ±zlÄ±, zstd = GÃ¼Ã§lÃ¼ sÄ±kÄ±ÅŸtÄ±rma)
ceph osd pool set vms compression_algorithm zstd
```

### B. Quality of Service (QoS) - HÄ±z Limitleme

"GÃ¼rÃ¼ltÃ¼lÃ¼ KomÅŸu" (Noisy Neighbor) sorununu Ã§Ã¶zmek iÃ§in. Test sunucusu production'Ä± yavaÅŸlatmasÄ±n.

```bash
# 1. Test diskine saniyede max 500 IOPS limiti koy
rbd config image set vms/test-vm-disk conf_rbd_qos_iops_limit 500

# 2. Veya bant geniÅŸliÄŸini (Bandwidth) sÄ±nÄ±rla (Ã–rn: 50 MB/s)
rbd config image set vms/test-vm-disk conf_rbd_qos_bps_limit 52428800
```

### C. AnÄ±nda Klonlama (Instant Cloning) - DevOps

10 TB'lÄ±k canlÄ± veritabanÄ±nÄ±n kopyasÄ±nÄ±, geliÅŸtiriciler iÃ§in **saniyeler iÃ§inde** ve **sÄ±fÄ±r yer kaplayarak** oluÅŸturun.

```bash
# 1. CanlÄ± diskin snapshot'Ä±nÄ± al (Saniyeler sÃ¼rer)
rbd snap create db_wal/postgres-prod@v1

# 2. Snapshot'Ä± "koru" (Silinmesini engelle)
rbd snap protect db_wal/postgres-prod@v1

# 3. GeliÅŸtirici-1 iÃ§in klon oluÅŸtur (Copy-On-Write, anÄ±nda biter)
rbd clone db_wal/postgres-prod@v1 db_wal/dev-db-1

# 4. GeliÅŸtirici-2 iÃ§in klon oluÅŸtur
rbd clone db_wal/postgres-prod@v1 db_wal/dev-db-2
```

*SonuÃ§: Elinizde birbirinden baÄŸÄ±msÄ±z, Ã¼zerinde Ã§alÄ±ÅŸÄ±labilir 2 yeni veritabanÄ± var ve diskte kapladÄ±klarÄ± yer baÅŸlagÄ±Ã§ta 0 KB.*

### D. Otomatik Katmanlama (Tiering) - Hot/Cold Data

SÄ±cak veriyi pahalÄ± NVMe'de, soÄŸuk veriyi ucuz HDD'de tut. (RGW Bucket Lifecycle ile).

1. **Storage Class YapÄ±landÄ±rmasÄ±:** RGW Ã¼zerinde `STANDARD` (NVMe) ve `COLD` (HDD) depolama sÄ±nÄ±flarÄ± tanÄ±mlanÄ±r.
2. **Ã–rnek Politika:**

```json
{
  "Rules": [{
    "ID": "MoveToColdStorage",
    "Status": "Enabled",
    "Filter": { "Prefix": "logs/" },
    "Transitions": [{
      "Days": 30,
      "StorageClass": "COLD"
    }]
  }]
}
```

---

## ğŸ¢ Senaryo 8: VMware vSphere Cluster'a Datastore SaÄŸlama

**Hedef:** VMware ESXi host'larÄ±na Ceph Ã¼zerinden paylaÅŸÄ±lan bir Datastore (veri deposu) vermek. Birden fazla ESXi host'un aynÄ± anda eriÅŸebildiÄŸi, HA yapÄ±labilir bir disk alanÄ± oluÅŸturmak.

### A. iSCSI Gateway ile (Ã–nerilen YÃ¶ntem)

```bash
# 1. iSCSI Gateway iÃ§in Ã¶zel havuz oluÅŸtur
ceph osd pool create vmware_iscsi 64 64
ceph osd pool application enable vmware_iscsi rbd

# 2. iSCSI Gateway servisini deploy et (3 node minimum Ã¶nerilir)
ceph orch apply iscsi vmware_iscsi api_user=admin api_password=Passw0rd \
    trusted_ip_list=192.168.1.0/24 \
    placement="node1,node2,node3"

# 3. iSCSI Target ve LUN oluÅŸtur (Admin UI'dan veya CLI ile)
# Dashboard'a giriÅŸ yap: https://ceph-mgr-ip:8443
# Block > iSCSI > Targets > Create
# - Target IQN: iqn.2024-01.com.ceph:vmware-datastore
# - LUN: vmware_ds1 (2TB boyutunda)
# - Initiator: ESXi host'larÄ±n IQN'lerini ekle

# 4. ESXi tarafÄ±nda (Her host'ta)
# Storage > Adapters > Software iSCSI > Enable
# Dynamic Discovery > Add Server: <ceph-iscsi-gateway-ip>:3260
# Rescan Storage
```

### B. RBD Direkt Mount (Alternatif - Proxmox tarzÄ±)

> **Not:** VMware resmi olarak RBD'yi desteklemiyor, ama Proxmox gibi KVM tabanlÄ± sistemlerde Ã§alÄ±ÅŸÄ±r.

```bash
# Proxmox iÃ§in:
pvesm add rbd vmware-storage --pool vmware_iscsi --content images
```

---

## ğŸªŸ Senaryo 9: Hyper-V Cluster'a CSV Storage SaÄŸlama

**Hedef:** Windows Hyper-V Failover Cluster iÃ§in Cluster Shared Volume (CSV) olarak kullanÄ±labilir bir Ceph depolama alanÄ± vermek.

### A. iSCSI Gateway ile (Ã–nerilen)

```bash
# 1. Hyper-V iÃ§in havuz oluÅŸtur
ceph osd pool create hyperv_iscsi 64 64
ceph osd pool application enable hyperv_iscsi rbd

# 2. iSCSI Gateway deploy et (Daha Ã¶nce yapÄ±ldÄ±ysa atla)
ceph orch apply iscsi hyperv_iscsi api_user=admin api_password=Passw0rd \
    trusted_ip_list=192.168.2.0/24

# 3. Target ve disk oluÅŸtur
# Dashboard > Block > iSCSI > Targets > Create
# - Target IQN: iqn.2024-01.com.ceph:hyperv-csv
# - Disk: hyperv_csv1 (5TB)
# - Initiator: Hyper-V node'larÄ±nÄ±n IQN'lerini ekle

# 4. Windows Hyper-V Node'larÄ±nda (Her node'da)
# iSCSI Initiator aÃ§Ä±lÄ±r
# Discovery Tab > Discover Portal: <ceph-iscsi-ip>:3260
# Targets Tab > Connect > Enable MPIO
# Disk Management > Online yap
# Failover Cluster Manager > Storage > Add Disk
# "Add to Cluster Shared Volumes" seÃ§
```

### B. SMB 3.0 ile (Alternatif - CephFS Ã¼zerinden)

> **Dikkat:** SMB Ã¼zerinden Hyper-V performansÄ±, iSCSI'den dÃ¼ÅŸÃ¼k olabilir. Kritik production iÃ§in iSCSI tercih edin.

```bash
# 1. CephFS volume oluÅŸtur
ceph fs volume create hyperv_smb

# 2. Samba Gateway deploy et (ÃœÃ§Ã¼ncÃ¼ parti araÃ§ gerekir)
# Ã–rnek: ceph-deploy veya manuel Samba kurulumu
# /etc/samba/smb.conf:
[hyperv_vms]
   path = /mnt/cephfs/hyperv
   read only = no
   vfs objects = ceph
   ceph:config_file = /etc/ceph/ceph.conf

# 3. Windows'ta
# Hyper-V Manager > Settings > Default Stores
# File Share Path: \\ceph-smb-gateway\hyperv_vms
```

---

## ğŸ¯ Senaryo Ã–zeti: Hangi YÃ¶ntemi SeÃ§meliyim?

| Platform | Ã–nerilen YÃ¶ntem | Performans | Kurulum KolaylÄ±ÄŸÄ± |
| :--- | :--- | :--- | :--- |
| **VMware vSphere** | iSCSI Gateway | â­â­â­â­ | Orta |
| **Hyper-V Cluster** | iSCSI Gateway | â­â­â­â­ | Orta |
| **Proxmox VE** | RBD (Native) | â­â­â­â­â­ | Kolay |
| **OpenStack** | RBD (Cinder) | â­â­â­â­â­ | Kolay |

---

## ğŸŒ Modern Ä°ÅŸ YÃ¼kleri ve GeliÅŸmiÅŸ Senaryolar

### ğŸŒ Senaryo 10: Global Veri Senkronizasyonu (Multi-site RGW)

**Problem:** Ä°stanbul ve Londra'daki iki farklÄ± ekip aynÄ± verilere hÄ±zlÄ± eriÅŸmeli ve bir lokasyon Ã§Ã¶kerse diÄŸeri Ã¼zerinden devam edilmeli.

**Ã‡Ã¶zÃ¼m:** RGW Multi-site (Active-Active) Replikasyon.

```bash
# 1. Zonegroup ve Zone yapÄ±landÄ±rmasÄ± (Ana Cluster'da)
# realm, zonegroup ve zone oluÅŸturulur
radosgw-admin realm create --rgw-realm=global_realm --default
radosgw-admin zonegroup create --rgw-zonegroup=turkiye --master --default
radosgw-admin zone create --rgw-zonegroup=turkiye --rgw-zone=istanbul --master --default

# 2. Senkronizasyon kullanÄ±cÄ±sÄ± oluÅŸtur
radosgw-admin user create --uid="syncuser" --display-name="Synchronization User" --system

# 3. Ä°kinci lokasyonda (Londra) bu realm bilgilerini Ã§ek ve zone oluÅŸtur
radosgw-admin realm pull --url=http://istanbul-ip:8000 --access-key=<ak> --secret-key=<sk>
radosgw-admin zone create --rgw-zonegroup=turkiye --rgw-zone=londra --default

# 4. DeÄŸiÅŸiklikleri uygula ve restart et
radosgw-admin period update --commit
```

### ğŸ§  Senaryo 11: AI / Derin Ã–ÄŸrenme Veri Besleme (High-Throughput)

**Problem:** GPU cluster'larÄ±, binlerce kÃ¼Ã§Ã¼k dosyadan oluÅŸan devasa dataset'leri (ImageNet vb.) aynÄ± anda Ã§ok hÄ±zlÄ± okumalÄ±.

**Ã‡Ã¶zÃ¼m:** CephFS Client Tuning ve MDS Ã–n Bellek Optimizasyonu.

```bash
# 1. MDS Cache boyutunu artÄ±r (RAM'iniz bolsa 8GB veya Ã¼stÃ¼)
ceph config set mds mds_cache_memory_limit 8589934592

# 2. Client tarafÄ±nda okuma hÄ±zÄ±nÄ± artÄ±rmak iÃ§in readahead ayarÄ±
mount -t ceph <mon_ip>:/ /mnt/ai_data -o name=admin,secret=<key>,rasize=67108864

# 3. KÃ¼Ã§Ã¼k dosyalar iÃ§in 'lazyio' Ã¶zelliÄŸini kullanarak eÅŸzamanlÄ±lÄ±ÄŸÄ± artÄ±rÄ±n (Uygulama desteÄŸi gerektirir)
```

### ğŸ“Š Senaryo 12: BÃ¼yÃ¼k Veri Log/Analitik Havuzu (Elasticsearch Backend)

**Problem:** Saniyede 50.000 log satÄ±rÄ± geliyor. Veri hem gÃ¼venli saklanmalÄ± hem de hÄ±zlÄ± indekslenmeli.

**Ã‡Ã¶zÃ¼m:** Replikasyonlu Index Havuzu + Erasure Coded Data Havuzu.

```bash
# 1. Metadata/Index iÃ§in SSD tabanlÄ± Replicated havuz (HÄ±zlÄ± arama)
ceph osd pool create log_index 32 32
ceph osd pool set log_index crush_rule rule-ssd

# 2. AsÄ±l veri (Data) iÃ§in HDD tabanlÄ± Erasure Coded havuz (Ucuz depolama)
ceph osd pool create log_data 128 128 erasure ec-profile-4-2

# 3. S3 katmanÄ±nda bu iki havuzu birleÅŸtirerek loglarÄ± saklayÄ±n.
```

### ğŸ› ï¸ Senaryo 13: CI/CD Ephemeral Storage (HÄ±zlÄ± Test OrtamlarÄ±)

**Problem:** Her yeni 'Build' iÃ§in 200GB'lÄ±k temiz bir OS diski lazÄ±m ama saniyeler iÃ§inde hazÄ±r olmalÄ±.

**Ã‡Ã¶zÃ¼m:** RBD Snapshot ve HÄ±zlÄ± Klonlama.

```bash
# 1. AltÄ±n imajÄ± (Gold Image) hazÄ±rla ve snap al
rbd snap create vms/ubuntu-22-template@gold
rbd snap protect vms/ubuntu-22-template@gold

# 2. Her yeni Job iÃ§in anlÄ±k klon oluÅŸtur (0 saniye sÃ¼rer)
rbd clone vms/ubuntu-22-template@gold vms/job-id-455-disk

# 3. Test bitince klonu anÄ±nda sil
rbd rm vms/job-id-455-disk
```

### ğŸ“º Senaryo 14: Media Sunucusu (Plex / Jellyfin / Emby)

**Problem:** Film, dizi ve mÃ¼zik koleksiyonunuzu tÃ¼m evdeki cihazlara (TV, tablet, telefon) akÄ±ÅŸla (stream) vermek istiyorsunuz. Binlerce video dosyasÄ± olacak ve birden fazla kiÅŸi aynÄ± anda izleyebilmeli.

**Ã‡Ã¶zÃ¼m:** CephFS ile YÃ¼ksek Throughput Media Storage.

```bash
# 1. Media iÃ§in yÃ¼ksek performanslÄ± CephFS volume oluÅŸtur
ceph fs volume create media_storage

# 2. Media sunucunuza mount edin
mkdir -p /mnt/media
mount -t ceph <mon_ip>:/ /mnt/media -o name=admin,secret=<key>,fs=media_storage

# 3. KlasÃ¶r yapÄ±sÄ± oluÅŸturun
mkdir -p /mnt/media/{movies,series,music,photos}

# 4. (Opsiyonel) Quota koyun (Ã–rn: 20TB)
setfattr -n ceph.quota.max_bytes -v 21990232555520 /mnt/media

# 5. Plex/Jellyfin'i kurun ve media klasÃ¶rÃ¼nÃ¼ gÃ¶sterin
# Plex Library Settings > Add Library > Browse > /mnt/media/movies
```

**Performans Ä°puÃ§larÄ±:**

```bash
# YÃ¼ksek okuma performansÄ± iÃ§in client tarafÄ±nda cache artÄ±rÄ±n
mount -t ceph <mon_ip>:/ /mnt/media -o rasize=67108864,name=admin,secret=<key>

# MDS'lerde cache'i artÄ±rÄ±n (4K transcoding iÃ§in)
ceph config set mds mds_cache_memory_limit 17179869184
```

**Neden CephFS?**

- **Ã‡oklu EriÅŸim:** Hem Plex hem de download araÃ§larÄ± (Sonarr/Radarr) aynÄ± anda yazabilir.
- **SÄ±nÄ±rsÄ±z BÃ¼yÃ¼me:** Koleksiyon bÃ¼yÃ¼dÃ¼kÃ§e sadece disk ekleyin.
- **Yedekleme Kolay:** CephFS snapshot ile anlÄ±k yedek alÄ±n.

---

## ğŸ›ï¸ Mimari Vizyon: Ceph Bir "Veri Ä°ÅŸletim Sistemi"dir

Bu reÃ§ete kitabÄ±ndaki 13 senaryoyu tek tek uyguladÄ±ÄŸÄ±nÄ±zda, elinizde sadece bir "depolama cihazÄ±" deÄŸil, komple bir **Veri AltyapÄ±sÄ± (Data Infrastructure)** oluÅŸur.

Ä°ÅŸte "BÃ¼yÃ¼k Resim" (The Big Picture):

| Katman | BileÅŸenler | Ceph KarÅŸÄ±lÄ±ÄŸÄ± |
| :--- | :--- | :--- |
| **1. Uygulama KatmanÄ±** | Oracle, SAP, Kubernetes, AI Model Training, Splunk, Video EditÃ¶rleri | *TÃ¼keticiler* |
| **2. Protokol KatmanÄ±** | Block (SAN), File (NAS), Object (S3), iSCSI, NFS | **RBD, CephFS, RGW** |
| **3. Veri Hizmetleri** | SÄ±kÄ±ÅŸtÄ±rma, Åifreleme, Snapshot, Klonlama, Tiering, WORM | **BlueStore Features** |
| **4. Fiziksel Katman** | NVMe, SSD, HDD, Network | **OSD, CRUSH Map** |

**Ã–zetle:**
ArtÄ±k firmanÄ±zdaki herhangi bir proje iÃ§in "Disk lazÄ±m" dendiÄŸinde, "Hangi marka storage alalÄ±m?" diye dÃ¼ÅŸÃ¼nmenize gerek yok. Cevap her zaman aynÄ±dÄ±r:
> *"Ceph cluster'Ä±nda yeni bir Pool aÃ§alÄ±m."*

---
**ğŸ‰ Tebrikler!** ArtÄ±k Ceph'i en basit disk ihtiyacÄ±ndan, en karmaÅŸÄ±k yapay zeka ve global replikasyon senaryolarÄ±na kadar yÃ¶netebilecek bir baÅŸucu rehberine (Cookbook) sahipsiniz.

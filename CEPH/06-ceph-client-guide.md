# ğŸš€ Ceph Ä°stemci KullanÄ±mÄ± ve Performans Testi (Client & Benchmark)

Bu dokÃ¼man, kurduÄŸumuz Ceph kÃ¼mesini "nasÄ±l kullanacaÄŸÄ±mÄ±zÄ±" ve "hÄ±zÄ±nÄ± nasÄ±l test edeceÄŸimizi" anlatÄ±r. Blok (RBD), Dosya (CephFS) ve Nesne (S3) eriÅŸim yÃ¶ntemleri ile hÄ±z testi senaryolarÄ±nÄ± iÃ§erir.

---

## ğŸ“‚ 1. CephFS (Dosya Sistemi) KullanÄ±mÄ±

CephFS, tÄ±pkÄ± bir NFS sunucusu gibi Ã§alÄ±ÅŸÄ±r. Birden fazla sunucu aynÄ± klasÃ¶re aynÄ± anda yazabilir (Shared File System).

### A. CephFS Volume OluÅŸturma (Admin TarafÄ±)

Daha Ã¶nce kurulumda MDS servisini aÃ§mÄ±ÅŸtÄ±k. Åimdi bir dosya sistemi yaratalÄ±m:

```bash
# Volume adÄ±: myfs
ceph fs volume create myfs
```

### B. Linux Ä°stemciye BaÄŸlama (Mount)

Ä°stemci makinede (Client) `ceph-common` yÃ¼klÃ¼ olmalÄ±dÄ±r.

**1. Secret Key'i Al:**
Admin sunucusunda ÅŸu komutla baÄŸlanma yetkisi (key) oluÅŸtur ve al:

```bash
ceph fs authorize myfs client.user1 / rw
# Ã‡Ä±ktÄ±daki anahtarÄ± kopyala (Ã–rn: AQAJz................==)
```

**2. Mount Et (Kernel Driver ile):**
> **Not:** En iyi performans ve uyumluluk iÃ§in Linux Kernel 5.x veya Ã¼zerini Ã¶neririm.
Ä°stemci makinede:

```bash
mkdir /mnt/mycephfs
# mon_ip: Node1 IP adresi
# secret: KopyaladÄ±ÄŸÄ±n anahtar
mount -t ceph 192.168.1.10:6789:/ /mnt/mycephfs -o name=user1,secret=AQAJz...==
```

ArtÄ±k `/mnt/mycephfs` klasÃ¶rÃ¼ne yazdÄ±ÄŸÄ±n her ÅŸey, arka planda Ceph cluster'Ä±na daÄŸÄ±tÄ±lÄ±r. `df -h` ile baktÄ±ÄŸÄ±nda Petabyte boyutunda bir alan gÃ¶rebilirsin.

---

## â˜ï¸ 2. S3 Object Storage (Rados Gateway) KullanÄ±mÄ±

Ceph'i kendi AWS S3'Ã¼nÃ¼z gibi kullanabilirsiniz.

### A. KullanÄ±cÄ± OluÅŸturma (Admin TarafÄ±)

Ã–nce bir S3 kullanÄ±cÄ±sÄ± yaratalÄ±m ve Access/Secret key Ã¼retelim.

```bash
radosgw-admin user create --uid=testuser --display-name="Test User"
```

**Ã‡Ä±ktÄ±daki ÅŸu deÄŸerleri kaydet:**

* `access_key`: (Ã–rn: J8X...)
* `secret_key`: (Ã–rn: 9bN...)

### B. Ä°stemci ile BaÄŸlanma (AWS CLI veya S3 Browser)

AWS CLI yÃ¼klÃ¼ bir makineden test edelim.

**1. AWS CLI YapÄ±landÄ±rmasÄ±:**

```bash
aws configure --profile myceph
# Access Key ve Secret Key'i gir.
# Region: default (veya us-east-1)
# Output: json
```

**2. Bucket OluÅŸturma ve Dosya Atma:**
Ceph RGW varsayÄ±lan olarak rastgele bir porttan (Ã¶rn: 8000, 8080) Ã§alÄ±ÅŸabilir.

```bash
# RGW Portunu Ã¶ÄŸren:
ceph orch ps --daemon_type rgw
# PORTS sÃ¼tununa bak (Ã–rn: *:8000)
```

AÅŸaÄŸÄ±daki komutlarda portu **8000** olarak varsayÄ±yoruz (kendi portunla deÄŸiÅŸtir):

```bash
# Bucket oluÅŸtur
aws --endpoint-url http://192.168.1.10:8000 s3 mb s3://testbucket --profile myceph

# Dosya yÃ¼kle
aws --endpoint-url http://192.168.1.10:8000 s3 cp deneme.txt s3://testbucket/ --profile myceph

# Listele
aws --endpoint-url http://192.168.1.10:8000 s3 ls s3://testbucket --profile myceph
```

---

## ğŸï¸ 3. Performans Testi (Benchmark)

"Sistemim kaÃ§ MB/s basÄ±yor?" sorusunun cevabÄ± iÃ§in Ceph'in kendi iÃ§inde gelen `rados bench` aracÄ±nÄ± kullanÄ±rÄ±z.

### Yazma Testi (Write Benchmark)

Cluster'Ä±n **yazma** kapasitesini Ã¶lÃ§er.

```bash
# testpool havuzuna, 10 saniye boyunca, durmaksÄ±zÄ±n veri yazar.
ceph osd pool create testpool 32 32
rados bench -p testpool 10 write --no-cleanup
```

**Ã‡Ä±ktÄ±da ÅŸuna bak:** `Bandwidth (MB/sec): [DeÄŸer]`

### Okuma Testi (Read Benchmark)

Az Ã¶nce yazÄ±lan verileri ne kadar hÄ±zlÄ± **okuyabiliyor**?

```bash
rados bench -p testpool 10 seq
```

### Temizlik

Test bittikten sonra Ã§Ã¶p verileri silmeyi unutma:

```bash
rados -p testpool cleanup
```

---

## ğŸ› ï¸ 4. GeliÅŸmiÅŸ Test: FIO (SanallaÅŸtÄ±rma Benzeri YÃ¼k)

`rados bench` ham disk performansÄ±nÄ± Ã¶lÃ§er. Ancak sanal makineler (VM) rastgele (random) okuma/yazma yapar. Bunu simÃ¼le etmek iÃ§in `fio` kullanÄ±lÄ±r.

Bir RBD diski (blok cihazÄ±) mount ettikten sonra ÅŸu komutu Ã§alÄ±ÅŸtÄ±r:

```bash
# 4k rastgele yazma testi (IOPS Ã¶lÃ§er)
fio --name=randwrite --ioengine=libaio --iodepth=1 --rw=randwrite --bs=4k --direct=1 --size=1G --numjobs=1 --runtime=60 --group_reporting --filename=/mnt/myrbd/testfile
```

* **IOPS:** Saniye baÅŸÄ±na iÅŸlem sayÄ±sÄ± (VeritabanlarÄ± iÃ§in en Ã¶nemli deÄŸer).
* **BW:** Bant geniÅŸliÄŸi (BÃ¼yÃ¼k dosya transferleri iÃ§in Ã¶nemli deÄŸer).

---

## ğŸ“¸ 5. RBD Ä°leri Seviye Ã–zellikler

### A. RBD Snapshot

```bash
# Snapshot oluÅŸtur
rbd snap create mypool/disk1@snap1

# Snapshot listele
rbd snap ls mypool/disk1

# Snapshot'tan geri yÃ¼kle (image'Ä± durdur Ã¶nce!)
rbd snap rollback mypool/disk1@snap1

# Snapshot sil
rbd snap rm mypool/disk1@snap1

# TÃ¼m snapshot'larÄ± sil
rbd snap purge mypool/disk1
```

### B. RBD Clone (Copy-on-Write)

```bash
# Snapshot'Ä± korumaya al (clone iÃ§in zorunlu)
rbd snap protect mypool/disk1@snap1

# Clone oluÅŸtur
rbd clone mypool/disk1@snap1 mypool/disk1-clone

# Clone'u baÄŸÄ±msÄ±z hale getir (flatten)
rbd flatten mypool/disk1-clone

# KorumayÄ± kaldÄ±r (clone silinmeli Ã¶nce)
rbd snap unprotect mypool/disk1@snap1
```

### C. RBD Resize

```bash
# BÃ¼yÃ¼tme (online yapÄ±labilir)
rbd resize mypool/disk1 --size 20G

# KÃ¼Ã§Ã¼ltme (DÄ°KKAT: veri kaybÄ±!)
rbd resize mypool/disk1 --size 5G --allow-shrink
```

### D. RBD Export/Import

```bash
# DÄ±ÅŸa aktar
rbd export mypool/disk1 /backup/disk1.raw

# Ä°Ã§e aktar
rbd import /backup/disk1.raw mypool/restored-disk
```

---

## ğŸ“ 6. CephFS Ä°leri Seviye Ã–zellikler

### A. CephFS Quota (Dizin Limiti)

```bash
# Dizin iÃ§in maksimum boyut (1 GB)
setfattr -n ceph.quota.max_bytes -v 1073741824 /mnt/mycephfs/project1

# Maksimum dosya sayÄ±sÄ±
setfattr -n ceph.quota.max_files -v 10000 /mnt/mycephfs/project1

# Quota'yÄ± gÃ¶rÃ¼ntÃ¼le
getfattr -n ceph.quota.max_bytes /mnt/mycephfs/project1

# Quota'yÄ± kaldÄ±r
setfattr -n ceph.quota.max_bytes -v 0 /mnt/mycephfs/project1
```

### B. CephFS Snapshot

```bash
# Snapshot dizini
mkdir /mnt/mycephfs/.snap/daily-backup

# Snapshot'larÄ± listele
ls /mnt/mycephfs/.snap/

# Snapshot'tan dosya geri yÃ¼kle
cp /mnt/mycephfs/.snap/daily-backup/myfile.txt /mnt/mycephfs/

# Snapshot sil
rmdir /mnt/mycephfs/.snap/daily-backup
```

### C. NFS-Ganesha (CephFS Ã¼zerinden NFS)

NFS-Ganesha, CephFS'i NFS protokolÃ¼ ile sunmanÄ±zÄ± saÄŸlar.

```bash
# NFS-Ganesha servisini daÄŸÄ±t
ceph orch apply nfs myfs-nfs --placement="node1"

# NFS export oluÅŸtur
ceph nfs export create cephfs myfs-nfs /cephfs myfs --path=/

# Export'u listele
ceph nfs export ls myfs-nfs

# Client'tan baÄŸlan (Linux)
mount -t nfs4 192.168.1.10:/cephfs /mnt/nfs-ceph
```

### D. FUSE vs Kernel Mount

| Ã–zellik | Kernel Mount | FUSE (ceph-fuse) |
| :--- | :--- | :--- |
| Performans | YÃ¼ksek | Orta |
| Stabilite | Kernel baÄŸÄ±mlÄ± | Daha esnek |
| Kurulum | Kernel modÃ¼lÃ¼ | Userspace |
| KullanÄ±m | Production | Test/Debug |

```bash
# FUSE ile mount
ceph-fuse -m 192.168.1.10:6789 /mnt/mycephfs --client_fs=myfs
```

---

## ğŸ”Œ 7. iSCSI Gateway (Windows/VMware iÃ§in)

Ceph RBD disk'lerini iSCSI protokolÃ¼ ile sunabilirsiniz.

### iSCSI Gateway Kurulumu

```bash
# iSCSI gateway servisini daÄŸÄ±t
ceph orch apply iscsi myiscsi mypool admin admin --placement="node1 node2"

# Dashboard Ã¼zerinden yapÄ±landÄ±rma:
# https://192.168.1.10:8443 â†’ Block â†’ iSCSI
```

### Windows Client BaÄŸlantÄ±sÄ±

1. Windows Server â†’ iSCSI Initiator
2. Target Portal: Ceph iSCSI Gateway IP
3. Target'a baÄŸlan
4. Disk Management'ta diski gÃ¶rÃ¼nÃ¼r yap

---

## ğŸŒ 8. RBD Mirroring (Site-to-Site Replikasyon)

Ä°ki farklÄ± veri merkezi arasÄ±nda RBD image'larÄ±nÄ± senkronize eder.

### Mirroring ModlarÄ±

| Mod | AÃ§Ä±klama | KullanÄ±m |
| :--- | :--- | :--- |
| `pool` | TÃ¼m pool senkronize | Basit DR |
| `image` | SeÃ§ili image'lar | GranÃ¼ler kontrol |

### Mirroring Kurulumu

**Site A (Primary):**

```bash
# Pool mirroring'i etkinleÅŸtir
rbd mirror pool enable mypool pool

# Peer ekle (Site B'nin baÄŸlantÄ± bilgisi)
rbd mirror pool peer add mypool client.admin@site-b --remote-mon-host 10.0.0.10
```

**Site B (Secondary):**

```bash
# rbd-mirror daemon'Ä± baÅŸlat
ceph orch apply rbd-mirror --placement="node1"

# Senkronizasyonu kontrol et
rbd mirror pool status mypool
```

### Failover

```bash
# Primary Ã§Ã¶ktÃ¼ÄŸÃ¼nde, Secondary'de:
rbd mirror image promote mypool/disk1

# Primary dÃ¼zeldiÄŸinde:
rbd mirror image demote mypool/disk1
rbd mirror image resync mypool/disk1
```

---

## ğŸ“Š 9. Client Best Practices

### âœ… YapÄ±lmasÄ± Gerekenler

* Her uygulama iÃ§in ayrÄ± RBD image kullanÄ±n

* DÃ¼zenli snapshot alÄ±n
* CephFS'te quota kullanÄ±n
* Kernel mount tercih edin (production iÃ§in)
* Mirroring ile DR planÄ± yapÄ±n

### âŒ YapÄ±lmamasÄ± Gerekenler

* Tek bir bÃ¼yÃ¼k image'a tÃ¼m veriyi koymayÄ±n

* Snapshot'larÄ± Ã§ok uzun tutmayÄ±n (alan dolar)
* Windows iÃ§in native kernel RBD client beklemeyin (iSCSI kullanÄ±n)
* NFS-Ganesha'yÄ± yÃ¼ksek performans gereken yerlerde kullanmayÄ±n

# ğŸ™ Ceph Master Guide: Nedir, Ne DeÄŸildir, NasÄ±l Ã‡alÄ±ÅŸÄ±r?

![Ceph Logo](https://raw.githubusercontent.com/faruk-guler/Sahara/refs/heads/main/CEPH/Images/ceph.webp)

Bu dokÃ¼man, Ceph depolama sistemini en temelinden en derin mimarisine kadar, bir sistem mÃ¼hendisinin bilmesi gereken detaylarla anlatmak iÃ§in hazÄ±rlanmÄ±ÅŸtÄ±r. Kurulumdan Ã¶nce "Neye bulaÅŸÄ±yoruz?" sorusunun tam cevabÄ±dÄ±r.

---

## 1. Ceph Nedir?

Ceph; **aÃ§Ä±k kaynaklÄ±**, **daÄŸÄ±tÄ±k (distributed)** ve **yazÄ±lÄ±m tabanlÄ± (software-defined)** bir depolama platformudur.

En bÃ¼yÃ¼k Ã¶zelliÄŸi **"Unified Storage" (BÃ¼tÃ¼nleÅŸik Depolama)** olmasÄ±dÄ±r. Yani dÃ¼nyadaki Ã¼Ã§ ana depolama yÃ¶ntemini tek bir kÃ¼mede (cluster) sunabilir:

1. **Blok Depolama (Block Storage):** Sanal makineler (VM) ve diskler iÃ§in (AWS EBS veya SAN gibi).
2. **Nesne Depolama (Object Storage):** BÃ¼yÃ¼k, yapÄ±sal olmayan veriler iÃ§in (AWS S3 gibi).
3. **Dosya Sistemi (File System):** KlasÃ¶r paylaÅŸÄ±mÄ± ve POSIX uyumlu eriÅŸim iÃ§in (NAS/NFS gibi).

### Ceph Ne DeÄŸildir?

* **Basit bir NAS deÄŸildir:** Evdeki QNAP veya Synology gibi "tak-Ã§alÄ±ÅŸtÄ±r" bir cihaz deÄŸildir.
* **RAID kullanmaz:** Geleneksel donanÄ±m RAID kartlarÄ±na (RAID 5, RAID 10 vb.) ihtiyaÃ§ duymaz, hatta onlardan nefret eder. Kendi yazÄ±lÄ±msal korumasÄ±nÄ± kullanÄ±r.
* **Tek sunuculuk iÅŸ deÄŸildir:** En az 3 sunucu ile gerÃ§ek gÃ¼cÃ¼nÃ¼ gÃ¶sterir. Tek sunucuda Ã§alÄ±ÅŸÄ±r ama Ceph'in mantÄ±ÄŸÄ±na aykÄ±rÄ±dÄ±r.

---

## 2. TarihÃ§e ve Ekosistem

"Bu teknoloji kimin eseri?" diye merak ediyorsanÄ±z, iÅŸte kÄ±sa bir tarihÃ§e:

* **Kurucu:** Sage Weil.
* **DoÄŸuÅŸ Yeri:** University of California, Santa Cruz (UCSC). Sage Weil'in doktora tezi olarak baÅŸladÄ± (2003-2007).
* **Ä°lk SÃ¼rÃ¼m:** 2006'da aÃ§Ä±k kaynak (LGPL) olarak yayÄ±nlandÄ±.
* **ÅirketleÅŸme:** 2012'de Sage Weil, Ceph'i geliÅŸtirmek iÃ§in **Inktank** ÅŸirketini kurdu.
* **BÃ¼yÃ¼k SatÄ±n Almalar:**
  * 2014: **Red Hat**, Inktank'i 175 Milyon Dolar'a satÄ±n aldÄ±.
  * 2019: **IBM**, Red Hat'i satÄ±n alarak teknolojinin en bÃ¼yÃ¼k hamisi oldu.
* **Ä°smin KÃ¶keni:** "Cephalopod" (Ahtapot, mÃ¼rekkep balÄ±ÄŸÄ± sÄ±nÄ±fÄ±) kelimesinden gelir. DaÄŸÄ±tÄ±k kollarÄ± olan, merkezi olmayan yapÄ±yÄ± temsil eder. UCSC'nin maskotu "Sammy" (bir Banana Slug) ile karÄ±ÅŸtÄ±rÄ±lsa da, logo ahtapot temasÄ±nÄ± iÅŸler.

BugÃ¼n Linux Ã§ekirdeÄŸinin (Kernel) yerleÅŸik bir parÃ§asÄ±dÄ±r ve CERN, Cisco, Bloomberg gibi dev yapÄ±lar tarafÄ±ndan kullanÄ±lmaktadÄ±r.

---

## 3. Temel Mimari: RADOS ve CRUSH

Ceph'in kalbinde **RADOS** (Reliable Autonomic Distributed Object Store) yatar. Her ÅŸeyi bu yÃ¶netir. Ãœstteki Blok, Nesne ve Dosya servisleri aslÄ±nda RADOS'un mÃ¼ÅŸterileridir.

### ğŸ§  CRUSH AlgoritmasÄ± (Sihirli DeÄŸnek)

Geleneksel depolamada "Dosya A nerede?" diye sorulduÄŸunda, merkezi bir veritabanÄ±na (Metadata Server) bakÄ±lÄ±r. Bu darboÄŸaz yaratÄ±r.

Ceph ise **CRUSH** (Controlled Replication Under Scalable Hashing) algoritmasÄ±nÄ± kullanÄ±r.

* **MantÄ±k:** Verinin nerede duracaÄŸÄ±nÄ± **hesaplar**, "sormaz".
* Ä°stemci (Client) matematiksel bir iÅŸlem yapar ve "Bu dosya Node 3'teki Disk 5'e gitmeli" der.
* Bu sayede merkezi bir darboÄŸaz (bottleneck) olmadan Exabyte'larca veriyi yÃ¶netebilir.

![Ceph Architecture Diagram](C:/Users/SISTEM/.gemini/antigravity/brain/85289e52-69c9-4c2e-9f4e-f6c356f0651c/ceph_architecture_diagram_1770194189443.png)

---

## 4. Ceph BileÅŸenleri (Diksiyonu)

Bir Ceph kÃ¼mesi ÅŸu 5 temel parÃ§adan oluÅŸur:

### 1. OSD (Object Storage Daemon) - "Ä°ÅŸÃ§iler"

* **GÃ¶revi:** Veriyi diske yazan, okuyan, Ã§oÄŸaltan ve disk bozulursa iyileÅŸtiren (recovery) servistir.
* **Kural:** Genelde her fiziksel disk (HDD/SSD) iÃ§in 1 adet OSD servisi Ã§alÄ±ÅŸÄ±r. 10 diskin varsa 10 OSD'n vardÄ±r.

### 2. MON (Monitor) - "Beyin TakÄ±mÄ±"

* **GÃ¶revi:** KÃ¼menin haritasÄ±nÄ± (Cluster Map) tutar. Kim ayakta, kim Ã§Ã¶ktÃ¼, veri nerede olmalÄ±?
* **Kural:** Tek sayÄ± olmak zorundadÄ±r (1, 3, 5). "Quorum" (Oylama) usulÃ¼ Ã§alÄ±ÅŸÄ±r. Ã‡oÄŸunluk saÄŸlanamazsa (Split-Brain) sistemi kilitler.

### 3. MGR (Manager) - "Ä°statistikÃ§i"

* **GÃ¶revi:** Performans metriklerini toplar, Dashboard'u sunar ve orkestrasyonu saÄŸlar.
* **Kural:** En az 1 aktif, 1 yedek (standby) olmasÄ± Ã¶nerilir.

### 4. MDS (Metadata Server) - "KÃ¼tÃ¼phaneci"

* **GÃ¶revi:** *Sadece CephFS (Dosya sistemi)* kullanÄ±yorsan gereklidir. Dosya isimleri, izinler ve dizin yapÄ±sÄ±nÄ± tutar.
* **Not:** Blok ve Object storage iÃ§in MDS gerekmez.

### 5. RGW (Rados Gateway) - "TercÃ¼man"

* **GÃ¶revi:** HTTP isteklerini (S3 veya Swift API) Ceph'in anlayacaÄŸÄ± dile (RADOS) Ã§evirir. Object Storage kullanacaksan gereklidir.

---

## 5. Veri NasÄ±l Korunur? (Replica vs Erasure Coding)

Ceph, verilerinizi kaybetmemek iÃ§in iki yÃ¶ntem sunar:

### A. Replikasyon (VarsayÄ±lan)

* Verinin kopyasÄ±nÄ± farklÄ± sunuculara yazar.
* **Ã–rnek (Size=3):** 1 GB veri yazarsan, fiziksel olarak 3 GB yer kaplar.
* **AvantajÄ±:** Ã‡ok hÄ±zlÄ±dÄ±r, iyileÅŸme (recovery) sÃ¼resi kÄ±sadÄ±r.
* **DezavantajÄ±:** PahalÄ±dÄ±r (Disk alanÄ±nÄ±n 3'te 1'ini kullanÄ±rsÄ±n).

### B. Erasure Coding (EC)

* RAID 5 veya RAID 6'nÄ±n matematiksel karÅŸÄ±lÄ±ÄŸÄ±dÄ±r. Veriyi parÃ§alar ve parite (koruma) kodlarÄ± ekler.
* **Ã–rnek (4+2):** Veriyi 4 parÃ§aya bÃ¶l, 2 tane de koruma parÃ§asÄ± ekle. Toplam 6 parÃ§a farklÄ± yerlere daÄŸÄ±lÄ±r.
* **AvantajÄ±:** Verimlidir (Disk alanÄ±nÄ±n %66'sÄ±nÄ± kullanÄ±rsÄ±n).
* **DezavantajÄ±:** Yazma iÅŸlemi yavaÅŸtÄ±r (CPU kullanÄ±r), iyileÅŸme sÃ¼resi uzundur. Genelde arÅŸiv/yedekleme iÃ§in kullanÄ±lÄ±r.

---

## 6. Ceph Trafik AkÄ±ÅŸÄ± (Life of an I/O)

Bir dosya yazmak istediÄŸinde arka planda ÅŸunlar olur:

1. **Ä°stemci:** DosyayÄ± (Object) havuza (Pool) atmak ister.
2. **Hash:** Dosya isminin Hash'ini alÄ±r.
3. **PG (Placement Group):** Hash sonucuna gÃ¶re dosyanÄ±n hangi PG'ye (Sanal Kova) gireceÄŸini bulur.
4. **CRUSH HesaplamasÄ±:** "Bu PG ÅŸu an hangi OSD'lerde (Disklerde) durmalÄ±?" sorusunu CRUSH algoritmasÄ± ile hesaplar.
5. **Yazma:** Ä°stemci, birincil (Primary) OSD'ye veriyi yazar.
6. **Replikasyon:** Birincil OSD, veriyi alÄ±r ve diÄŸer 2 kopya OSD'ye (Secondary) gÃ¶nderir (varsayÄ±lan size=3 iÃ§in).
7. **Onay (Ack):** DiÄŸer 2 OSD "YazdÄ±m" dediÄŸinde, Birincil OSD istemciye "Ä°ÅŸlem Tamam" der.
    * *Bu sayede veri tutarlÄ±lÄ±ÄŸÄ± (consistency) %100 garanti altÄ±na alÄ±nÄ±r.*

![Ceph Data Flow Diagram](C:/Users/SISTEM/.gemini/antigravity/brain/85289e52-69c9-4c2e-9f4e-f6c356f0651c/ceph_data_flow_1770194214667.png)

---

## 7. Neden Ceph? (Use Cases)

### Ne Zaman KullanmalÄ±sÄ±n?

* **OpenStack / Kubernetes / Proxmox:** Sanal makineleriniz iÃ§in ortak, paylaÅŸÄ±mlÄ± ve bÃ¼yÃ¼yebilen bir disk alanÄ± lazÄ±msa.
* **S3 Depolama Ä°htiyacÄ±:** Kendi verimerkezinde AWS S3 gibi bir yapÄ± kurmak istiyorsan (MinIO alternatifi olarak daha enterprise).
* **PetaByte Ã–lÃ§eÄŸi:** 100 TB ve Ã¼zeri verin varsa, geleneksel NAS'lar yetmez. Ceph sÄ±nÄ±rsÄ±z bÃ¼yÃ¼r (Scale-out).

### Ne Zaman KullanmamalÄ±sÄ±n?

* **Ã‡ok kÃ¼Ã§Ã¼k kurulumlar:** "3 tane diskim var, dosya sunucusu yapacaÄŸÄ±m" diyorsan TrueNAS kur. Ceph bu Ã¶lÃ§ek iÃ§in fazla komplekstir.
* **YÃ¼ksek PerformanslÄ± Database (Latency):** Ã‡ok dÃ¼ÅŸÃ¼k gecikme (latency) gerektiren veritabanlarÄ± iÃ§in (Oracle RAC vb.) NVMe All-Flash Ceph kurmuyorsan, geleneksel SAN daha hÄ±zlÄ± olabilir.

---

## 8. Ã–zet

Ceph, donanÄ±m baÄŸÄ±msÄ±zlÄ±ÄŸÄ± sunan, kendi kendini yÃ¶netebilen ve iyileÅŸtirebilen **"GeleceÄŸin Depolama teknolojisidir"**. Ã–ÄŸrenme eÄŸrisi diktir (zordur), ancak bir kez kavradÄ±ÄŸÄ±nÄ±zda veri merkezinizin en gÃ¼venilir parÃ§asÄ± olur. DonanÄ±m bozulur, diskler yanar, sunucular Ã§Ã¶ker; ama **Ceph hayatta kalÄ±r.**

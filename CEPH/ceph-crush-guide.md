# ğŸ—ºï¸ Ceph CRUSH Map YÃ¶netimi Rehberi

CRUSH (Controlled Replication Under Scalable Hashing), Ceph'in verinin nerede depolanacaÄŸÄ±nÄ± belirleyen algoritmasÄ±dÄ±r. Bu rehber, CRUSH haritasÄ±nÄ±n anlaÅŸÄ±lmasÄ± ve Ã¶zelleÅŸtirilmesini kapsar.

---

## 1. CRUSH Temelleri

### CRUSH Nedir?

CRUSH, merkezi bir metadata sunucusuna ihtiyaÃ§ duymadan verinin hangi OSD'lerde saklanacaÄŸÄ±nÄ± hesaplayan deterministik bir algoritmadÄ±r.

**AvantajlarÄ±:**

- Merkezi darboÄŸaz yok
- Sonsuz Ã¶lÃ§eklenebilirlik
- Deterministik (aynÄ± girdi = aynÄ± Ã§Ä±ktÄ±)
- Failure domain kontrolÃ¼

### CRUSH HiyerarÅŸisi (Bucket TÃ¼rleri)

```text
root (kÃ¶k)
 â””â”€â”€ datacenter (veri merkezi)
      â””â”€â”€ room (oda)
           â””â”€â”€ rack (dolap)
                â””â”€â”€ host (sunucu)
                     â””â”€â”€ osd (disk)
```

---

## 2. CRUSH Map GÃ¶rÃ¼ntÃ¼leme

### Mevcut HaritayÄ± GÃ¶rme

```bash
# CRUSH aÄŸacÄ±nÄ± gÃ¶rÃ¼ntÃ¼le
ceph osd tree

# CRUSH haritasÄ±nÄ± dÄ±ÅŸa aktar (binary)
ceph osd getcrushmap -o crushmap.bin

# Binary'yi okunabilir formata Ã§evir
crushtool -d crushmap.bin -o crushmap.txt

# HaritayÄ± gÃ¶rÃ¼ntÃ¼le
cat crushmap.txt
```

### OSD KonumlarÄ±nÄ± GÃ¶rme

```bash
ceph osd crush dump
```

---

## 3. CRUSH Map DÃ¼zenleme

### YÃ¶ntem 1: CLI ile AnlÄ±k DeÄŸiÅŸiklik

```bash
# OSD'yi farklÄ± bir host'a taÅŸÄ±
ceph osd crush set osd.5 1.0 host=node3

# Yeni bucket (host) oluÅŸtur
ceph osd crush add-bucket newhost host

# Host'u rack'e baÄŸla
ceph osd crush move newhost rack=rack1

# Rack oluÅŸtur ve root'a baÄŸla
ceph osd crush add-bucket rack2 rack
ceph osd crush move rack2 root=default
```

### YÃ¶ntem 2: Map DosyasÄ±nÄ± DÃ¼zenleyerek

```bash
# 1. Mevcut haritayÄ± al
ceph osd getcrushmap -o crushmap.bin
crushtool -d crushmap.bin -o crushmap.txt

# 2. DÃ¼zenle
nano crushmap.txt

# 3. Tekrar derle ve uygula
crushtool -c crushmap.txt -o newcrushmap.bin
ceph osd setcrushmap -i newcrushmap.bin
```

---

## 4. Failure Domain (Hata AlanÄ±)

Failure domain, Ceph'in replikalarÄ± nasÄ±l daÄŸÄ±ttÄ±ÄŸÄ±nÄ± belirler.

### VarsayÄ±lan DavranÄ±ÅŸ

```text
VarsayÄ±lan: failure-domain = host
â†’ Her replika farklÄ± bir sunucuda tutulur
â†’ Bir sunucu bozulursa veri kaybolmaz
```

### Failure Domain Seviyeleri

| Seviye | Koruma | Gereksinim |
| :--- | :--- | :--- |
| `osd` | Disk arÄ±zasÄ± | En az 3 OSD |
| `host` | Sunucu arÄ±zasÄ± | En az 3 sunucu |
| `rack` | Dolap arÄ±zasÄ± | En az 3 rack |
| `datacenter` | DC arÄ±zasÄ± | En az 3 veri merkezi |

### CRUSH Rule ile Failure Domain DeÄŸiÅŸtirme

```bash
# Host bazlÄ± kural (varsayÄ±lan)
ceph osd crush rule create-replicated host-rule default host

# Rack bazlÄ± kural
ceph osd crush rule create-replicated rack-rule default rack

# Pool'a uygula
ceph osd pool set mypool crush_rule rack-rule
```

---

## 5. OSD SÄ±nÄ±flarÄ± (Device Classes)

Ceph, disk tÃ¼rlerini otomatik algÄ±lar ve sÄ±nÄ±flandÄ±rÄ±r.

### SÄ±nÄ±flarÄ± GÃ¶rme

```bash
ceph osd crush class ls
# Ã‡Ä±ktÄ±: [hdd, ssd, nvme]

# OSD'lerin sÄ±nÄ±flarÄ±nÄ± gÃ¶r
ceph osd crush tree --show-shadow
```

### SÄ±nÄ±f BazlÄ± CRUSH KuralÄ±

```bash
# Sadece SSD'leri kullanan kural
ceph osd crush rule create-replicated ssd-only default host ssd

# Sadece HDD'leri kullanan kural
ceph osd crush rule create-replicated hdd-only default host hdd

# Pool'lara uygula
ceph osd pool set fast-pool crush_rule ssd-only
ceph osd pool set archive-pool crush_rule hdd-only
```

### Manuel SÄ±nÄ±f Atama

```bash
# OSD'nin sÄ±nÄ±fÄ±nÄ± kaldÄ±r
ceph osd crush rm-device-class osd.5

# Yeni sÄ±nÄ±f ata
ceph osd crush set-device-class nvme osd.5
```

---

## 6. Custom CRUSH Rules

### Kural YapÄ±sÄ±

```text
rule <rule-name> {
    id <unique-id>
    type replicated
    min_size <minimum-replicas>
    max_size <maximum-replicas>
    step take <bucket>
    step chooseleaf firstn <count> type <failure-domain>
    step emit
}
```

### Ã–rnek: Rack BazlÄ± Kural

```bash
# CRUSH haritasÄ±nÄ± dÃ¼zenle
crushtool -d crushmap.bin -o crushmap.txt
```

Dosyaya ekle:

```
rule rack-isolation {
    id 10
    type replicated
    min_size 1
    max_size 10
    step take default
    step chooseleaf firstn 0 type rack
    step emit
}
```

```bash
# Derle ve uygula
crushtool -c crushmap.txt -o newcrushmap.bin
ceph osd setcrushmap -i newcrushmap.bin
```

---

## 7. Stretched Cluster (Ã‡oklu Veri Merkezi)

### YapÄ±

```text
root
â”œâ”€â”€ datacenter-a
â”‚   â””â”€â”€ host-a1, host-a2
â”œâ”€â”€ datacenter-b
â”‚   â””â”€â”€ host-b1, host-b2
â””â”€â”€ datacenter-arbiter
    â””â”€â”€ host-arbiter (sadece MON iÃ§in)
```

### Stretched Mode AktifleÅŸtirme

```bash
# Stretch mode iÃ§in MON'larÄ± yapÄ±landÄ±r
ceph mon enable_stretch_mode arbiter-host dc datacenter-a datacenter-b
```

---

## 8. CRUSH Tunables

CRUSH versiyonu ve davranÄ±ÅŸÄ±nÄ± kontrol eder.

```bash
# Mevcut tunables'Ä± gÃ¶r
ceph osd crush show-tunables

# Optimal profile'a gÃ¼ncelle (dikkatli ol!)
ceph osd crush tunables optimal
```

### Tunable Profilleri

| Profil | Ceph Versiyonu | Ã–zellik |
| :--- | :--- | :--- |
| `legacy` | Bobtail Ã¶ncesi | Eski, kÃ¶tÃ¼ daÄŸÄ±lÄ±m |
| `bobtail` | Bobtail | Temel iyileÅŸtirmeler |
| `hammer` | Hammer | Daha iyi dengeleme |
| `optimal` | Luminous+ | En iyi daÄŸÄ±lÄ±m |

---

## 9. OSD AÄŸÄ±rlÄ±klarÄ± (Weights)

### CRUSH Weight

Disk kapasitesini temsil eder (TB cinsinden).

```bash
# Weight'i gÃ¶rÃ¼ntÃ¼le
ceph osd crush tree

# Weight deÄŸiÅŸtir
ceph osd crush reweight osd.5 2.0  # 2 TB disk iÃ§in
```

### OSD Reweight (GeÃ§ici AÄŸÄ±rlÄ±k)

OSD doluluk dengesizliÄŸini dÃ¼zeltmek iÃ§in kullanÄ±lÄ±r.

```bash
# 0.0 ile 1.0 arasÄ± deÄŸer (1.0 = tam kapasite)
ceph osd reweight osd.5 0.8

# Otomatik dengeleme
ceph osd reweight-by-utilization
```

---

## 10. CRUSH SimÃ¼lasyonu ve Test

### Veri DaÄŸÄ±lÄ±mÄ±nÄ± SimÃ¼le Et

```bash
# Bir pool iÃ§in veri daÄŸÄ±lÄ±mÄ±nÄ± test et
crushtool -i crushmap.bin --test --show-mappings --rule 0 --num-rep 3
```

### Kural DeÄŸiÅŸikliÄŸinin Etkisini Ã–nceden GÃ¶r

```bash
# KaÃ§ PG hareket edecek?
ceph osd getmap -o osdmap.bin
osdmaptool osdmap.bin --test-map-pgs --pool mypool
```

---

## 11. CRUSH Best Practices

### âœ… YapÄ±lmasÄ± Gerekenler

- Failure domain'i iÅŸ yÃ¼kÃ¼ne gÃ¶re seÃ§in
- SSD ve HDD'leri ayrÄ± device class'larda tutun
- CRUSH deÄŸiÅŸikliklerini lab ortamÄ±nda test edin
- BÃ¼yÃ¼k deÄŸiÅŸiklikler Ã¶ncesi `ceph osd set norebalance` kullanÄ±n

### âŒ YapÄ±lmamasÄ± Gerekenler

- Production'da test edilmemiÅŸ CRUSH map uygulamayÄ±n
- TÃ¼m OSD'leri aynÄ± host'a atamayÄ±n (failure domain ihlali)
- Tunables'Ä± gereksiz yere deÄŸiÅŸtirmeyin
- Manual weight deÄŸiÅŸikliklerini aÅŸÄ±rÄ± kullanmayÄ±n

---

## 12. YaygÄ±n CRUSH SorunlarÄ± ve Ã‡Ã¶zÃ¼mleri

### Sorun: Uneven Data Distribution

```bash
# KullanÄ±m oranlarÄ±nÄ± kontrol et
ceph osd df tree

# Otomatik reweight
ceph osd reweight-by-utilization

# Balancer modÃ¼lÃ¼nÃ¼ etkinleÅŸtir
ceph balancer on
ceph balancer mode upmap
```

### Sorun: PG Stuck in Active+Remapped

```bash
# CRUSH kuralÄ±nÄ± kontrol et
ceph osd pool get mypool crush_rule

# Yeterli OSD var mÄ±?
ceph osd tree
```

### Sorun: HEALTH_WARN - Failed to choose

```bash
# CRUSH rule ve failure domain uyumsuzluÄŸu
# Host sayÄ±sÄ± < replica sayÄ±sÄ± olabilir
ceph osd pool set mypool size 2  # veya daha fazla host ekle
```
